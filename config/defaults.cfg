#**************************************************************
[DEFAULT]
LANG = English
LC = en
TREEBANK = EWT
TB = ewt
save_metadir = saves
ElmoNetwork_dir = %(save_metadir)s/%(LANG)s-%(TREEBANK)s/Elmo
TaggerNetwork_dir = %(save_metadir)s/%(LANG)s-%(TREEBANK)s/Tagger
ParserNetwork_dir = %(save_metadir)s/%(LANG)s-%(TREEBANK)s/Parser
network_class = 
save_dir = %(save_metadir)s/%(LANG)s-%(TREEBANK)s/%(network_class)s
train_conllus = data/CoNLL18/UD_%(LANG)s-%(TREEBANK)s/%(LC)s_%(TB)s-ud-train.conllu
dev_conllus = data/CoNLL18/UD_%(LANG)s-%(TREEBANK)s/%(LC)s_%(TB)s-ud-dev.conllu
test_conllus = %(dev_conllus)s

#***************************************************************
# Network
[Config]

[BaseNetwork]
n_passes = 0
max_steps = 30000
max_steps_without_improvement = 3000
print_every = 100
save_model = True
parse_datasets = False
switch_optimizers = False
# neural
l2_reg = 0
output_keep_prob = .67
conv_keep_prob = .67
recur_keep_prob = .67
recur_include_prob = 1.
hidden_keep_prob = .67
n_layers = 3
first_layer_conv_width = 0
conv_width = 0
output_size = 100
recur_size = 300
output_func = identity
bidirectional = True
recur_cell = LSTM
recur_func = tanh
cifg = False
highway = True
highway_func = identity
bilin = False

[ElmoNetwork]
input_vocab_classes = FormSubtokenVocab
output_vocab_classes = FormTokenVocab
throughput_vocab_classes = LemmaTokenVocab
input_network_classes = None
#neural
recur_size = 500
n_layers = 2

[TaggerNetwork]
input_vocab_classes = FormMultivocab
output_vocab_classes = UPOSTokenVocab:XPOSTokenVocab
throughput_vocab_classes = LemmaTokenVocab
input_network_classes = None
#neural
n_layers = 2

[ParserNetwork]
input_vocab_classes = FormMultivocab:UPOSTokenVocab:XPOSTokenVocab:LemmaTokenVocab
output_vocab_classes = DepheadIndexVocab:DeprelTokenVocab
throughput_vocab_classes = None
input_network_classes = None
sum_pos = False

[GraphOutputs]

#**************************************************************
# CoNLLU fields
[CoNLLUVocab]

[FormVocab]
[LemmaVocab]
[UPOSVocab]
[XPOSVocab]
[DepheadVocab]
[DeprelVocab]
[SemrelVocab]
[SemheadVocab]

#***************************************************************
# Datasets
[CoNLLUDataset]
max_buckets = 5
batch_size = 10000

[CoNLLUTrainset]
max_buckets = 15
batch_size = 5000

[CoNLLUDevset]

[CoNLLUTestset]

#**************************************************************
# Vocabulary types

#===============================================================
# Numeric vocabs
[IndexVocab]
#neural
hidden_size = 600
hidden_keep_prob = .67
add_linear = True
n_layers = 1
hidden_func = leaky_relu
diagonal = False
linearize = False
distance = False

[IDIndexVocab]

[DepheadIndexVocab]

[SemheadGraphIndexVocab]

#===============================================================
# String Vocabs
[SetVocab]
cased = None
special_token_case = None
special_token_html = None

[PretrainedVocab]
cased = False
special_token_case = upper
special_token_html = True
max_embed_count = 0
pretrained_file = None
name = None
# neural
linear_size = 100
embed_keep_prob = .67

[FormPretrainedVocab]
pretrained_file = data/word2vec/%(LANG)s/%(LC)s.vectors.xz
name = word2vec

#===============================================================
# Token vocabs
[CountVocab]
cased = None
min_occur_count = None

[TokenVocab]
cased = True
special_token_case = upper
special_token_html = True
min_occur_count = 1
# neural
embed_size = 100
embed_keep_prob = .67
drop_func = unkout
hidden_size = 200
hidden_keep_prob = .67
n_layers = 1
hidden_func = leaky_relu

[FormTokenVocab]
cased = False
min_occur_count = 5

[LemmaTokenVocab]
cased = False
min_occur_count = 5

[UPOSTokenVocab]
special_token_html = False
embed_size=50

[XPOSTokenVocab]
special_token_html = False
embed_size=50

[DeprelTokenVocab]
special_token_case = lower
special_token_html = False
factorized = True
# neural
diagonal = False
add_linear = True
loss_interpolation = .5

[SemrelGraphTokenVocab]
special_token_case = lower
special_token_html = False
factorized = True
# neural
add_linear = True
loss_interpolation = .033

#===============================================================
# Subtoken vocabs
[SubtokenVocab]
cased = True
special_token_case = upper
special_token_html = True
min_occur_count = 1
max_buckets = 2
# neural
embed_size = 100
embed_keep_prob = 1.
conv_keep_prob = .67
recur_keep_prob = .67
recur_include_prob = 1.
output_keep_prob = .67
n_layers = 1
first_layer_conv_width = 0
conv_width = 0
recur_size = 300
bidirectional = False
recur_cell = LSTM
recur_func = tanh
output_func = identity
cifg = False
highway = False
highway_func = identity
bilin = False
squeeze_type = linear_attention
output_size = 100

[FormSubtokenVocab]
min_occur_count = 10

[LemmaSubtokenVocab]
min_occur_count = 10

#===============================================================
# Multivocabs
# TODO rework multivocabs
[Multivocab]
use_token_vocab = True
use_subtoken_vocab = False
use_pretrained_vocab = True
pretrained_files = None
names = None
# neural
combine_func = concat
embed_keep_prob = .67
drop_func = unkout

[FormMultivocab]
use_token_vocab = True
use_pretrained_vocab = True
use_subtoken_vocab = True

#***************************************************************
# Optimization
[Optimizer]
learning_rate = .002
decay_rate = 0
clip = 1.
mu = .75
nu = .95
epsilon = 1e-12
gamma = 0

[AMSGradOptimizer]
mu = .9
nu = .95
